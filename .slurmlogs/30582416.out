Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 12
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	findMarkers
	1

[Mon Feb 12 15:05:32 2024]
rule findMarkers:
    input: data/processed/single_cell/merged_no_doublets.h5seurat, data/processed/bulk/all_counts.csv
    output: data/processed/single_cell/celltype_labeled.h5seurat, data/processed/single_cell/cluster_markers.csv
    jobid: 0


The following have been reloaded with a version change:
  1) r/4.3.1 => r/4.2.1

/usr/bin/bash: line 1: Rscript scripts/5_findMarkers.R: No such file or directory
[Mon Feb 12 15:05:33 2024]
Error in rule findMarkers:
    jobid: 0
    output: data/processed/single_cell/celltype_labeled.h5seurat, data/processed/single_cell/cluster_markers.csv
    shell:
        
        module load r r/4.2.1
        "Rscript scripts/5_findMarkers.R"
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
